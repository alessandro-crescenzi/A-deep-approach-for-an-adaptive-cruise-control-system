{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandro-crescenzi/computer_vision_project/blob/main/Traffic_Sign_Training_(last%20version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfrWUTSpIbQ5"
      },
      "source": [
        "Download the dataset from: https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/published-archive.html\n",
        "\n",
        "The training set is: GTSRB_Online-Test-Images-Sorted.zip\n",
        "The test set is: GTSRB_Online-Test-Images-Sorted.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghFGzyt5_V5w",
        "outputId": "552dae9f-a467-4ee3-ecc6-9b6565a1d119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets, models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mount_folder = '/content/drive/My Drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wYEUDSBwTqC",
        "outputId": "d61c8f5c-b3a4-44fc-8709-9419d1246d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/train_images/GTSRB/Final_Training/Images': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive/train_images/GTSRB/Final_Training/Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqavycQg4Vl3"
      },
      "outputs": [],
      "source": [
        "!ls '/content/drive/My Drive/train_images/GTSRB/Final_Training/Images/00000'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6DXrtxtD9SF",
        "outputId": "b0b0b3ac-6788-4b13-c589-45209d2b8c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive/test_images/GTSRB/Online-Test-sort/00000'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhD0D_tmO_vM",
        "outputId": "08d02ed4-7565-492a-98e3-bf864dcb17e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".  ..  0\n"
          ]
        }
      ],
      "source": [
        "!ls -a '/content/prova'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "jVtWCvb6EuTY"
      },
      "outputs": [],
      "source": [
        "rm -rf '/content/prova/.ipynb_checkpoints'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vakJX1NRQNK_",
        "outputId": "a0c53dcb-9001-483a-bbff-687c502e6186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "m2sm0HnuHr0c"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "train_zip = mount_folder + '/GTSRB_Final_Training_Images.zip'\n",
        "test_zip = mount_folder + '/GTSRB_Online-Test-Images-Sorted.zip'\n",
        "\n",
        "if not os.path.exists(train_zip) or not os.path.exists(test_zip):\n",
        "    raise(RuntimeError(\"Could not find \" + train_zip + \" and/or \" + test_zip\n",
        "          + ', download them from https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/published-archive.html'))\n",
        "    \n",
        "# Extract train_data.zip\n",
        "train_folder = mount_folder + '/train_images'\n",
        "if not os.path.isdir(train_folder):\n",
        "    print(train_folder + ' not found, extracting ' + train_zip)\n",
        "    zip_ref = zipfile.ZipFile(train_zip, 'r')\n",
        "    zip_ref.extractall(train_folder)\n",
        "    zip_ref.close()\n",
        "    \n",
        "# Extract test_data.zip\n",
        "test_folder = mount_folder + '/test_images'\n",
        "if not os.path.isdir(test_folder):\n",
        "    print(test_folder + ' not found, extracting ' + test_zip)\n",
        "    zip_ref = zipfile.ZipFile(test_zip, 'r')\n",
        "    zip_ref.extractall(test_folder)\n",
        "    zip_ref.close()\n",
        "\n",
        "train_folder= train_folder + '/GTSRB/Final_Training/Images'\n",
        "#new_train_folder = mount_folder + '/GTSRB_train'\n",
        "\n",
        "test_folder= test_folder + '/GTSRB/Online-Test-sort'\n",
        "folder_to_remove = test_folder + '/Images'\n",
        "#new_test_folder= test_folder + '/GTSRB/Online-Test-sort'\n",
        "\n",
        "!rm -rf \"$folder_to_remove\"\n",
        "\n",
        "#!mv \"$train_folder\" \"$new_train_folder\"\n",
        "#!mv \"$test_folder\" \"$new_train_folder\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7FR9fHMk6dn"
      },
      "source": [
        "classes = { 0:'Speed limit (20km/h)',\n",
        "            1:'Speed limit (30km/h)', \n",
        "            2:'Speed limit (50km/h)', \n",
        "            3:'Speed limit (60km/h)', \n",
        "            4:'Speed limit (70km/h)', \n",
        "            5:'Speed limit (80km/h)', \n",
        "            6:'End of speed limit (80km/h)', \n",
        "            7:'Speed limit (100km/h)', \n",
        "            8:'Speed limit (120km/h)', \n",
        "            9:'No passing', \n",
        "            10:'No passing veh over 3.5 tons', \n",
        "            11:'Right-of-way at intersection', \n",
        "            12:'Priority road', \n",
        "            13:'Yield', \n",
        "            14:'Stop', \n",
        "            15:'No vehicles', \n",
        "            16:'Veh > 3.5 tons prohibited', \n",
        "            17:'No entry', \n",
        "            18:'General caution', \n",
        "            19:'Dangerous curve left', \n",
        "            20:'Dangerous curve right', \n",
        " \t\t        21:'Double curve', \n",
        "            22:'Bumpy road', \n",
        "            23:'Slippery road', \n",
        "            24:'Road narrows on the right', \n",
        "            25:'Road work', \n",
        "            26:'Traffic signals', \n",
        "            27:'Pedestrians', \n",
        "            28:'Children crossing', \n",
        "            29:'Bicycles crossing', \n",
        "            30:'Beware of ice/snow',\n",
        "            31:'Wild animals crossing', \n",
        "            32:'End speed + passing limits', \n",
        "            33:'Turn right ahead', \n",
        "            34:'Turn left ahead', \n",
        "            35:'Ahead only', \n",
        "            36:'Go straight or right', \n",
        "            37:'Go straight or left', \n",
        "            38:'Keep right', \n",
        "            39:'Keep left', \n",
        "            40:'Roundabout mandatory', \n",
        "            41:'End of no passing', \n",
        "            42:'End no passing veh > 3.5 tons' }\n",
        "\n",
        "We are only interested in recognizing speed limits and stop signs for our purposes (classes 0-8, 13, 14, 32), we set the others to a unique class \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "daWdPxzWmVSl"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "'''\n",
        "#Remove leading zeros in Train Data directories \n",
        "\n",
        "for folder_name in os.listdir(train_folder):\n",
        "    if folder_name == '00000' or folder_name == '0':\n",
        "       clean = '0'\n",
        "    else:\n",
        "       clean = folder_name.lstrip('0')\n",
        "\n",
        "    if (folder_name != clean):\n",
        "        os.rename (os.path.join(train_folder, folder_name),os.path.join(train_folder, clean))\n",
        "\n",
        "#Remove leading zeros in Test Data directories \n",
        "\n",
        "for folder_name in os.listdir(test_folder):\n",
        "    if folder_name == '00000' or folder_name == '0':\n",
        "       clean = '0'\n",
        "    else:\n",
        "       clean = folder_name.lstrip('0')\n",
        "\n",
        "    if (folder_name != clean):\n",
        "        os.rename (os.path.join(test_folder, folder_name),os.path.join(test_folder, clean))\n",
        "'''\n",
        "\n",
        "mapping = { \"00000\": \"00000\", \"00001\": \"00001\", \"00002\": \"00002\", \"00003\": \"00003\", \"00004\": \"00004\", \"00005\": \"00005\", \"00006\": \"00006\", \"00007\": \"00007\", \"00008\": \"00008\", \"00013\": \"00009\", \"00014\": \"00010\", \"00032\": \"00011\" }\n",
        "unknown = \"00012\"\n",
        "\n",
        "#Append name of folder to every file in training folder, then apply mapping \n",
        "for folder_name in os.listdir(train_folder):\n",
        "\n",
        "    folder_path = os.path.join(train_folder, folder_name)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "      \n",
        "      for file_name in os.listdir(folder_path):\n",
        "          file_extension = pathlib.Path(file_name).suffix\n",
        "          file_no_extension = file_name.replace(file_extension, \"\")\n",
        "          #os.rename (os.path.join(folder_path, file_name),os.path.join(folder_path, file_no_extension + \"_\" + folder_name + file_extension))\n",
        "\n",
        "          if mapping.get(folder_name) is not None:\n",
        "            os.rename (os.path.join(folder_path, file_name),os.path.join(os.path.join(train_folder, mapping[folder_name]), file_no_extension + \"_\" + folder_name + file_extension))\n",
        "          else:\n",
        "            os.rename (os.path.join(folder_path, file_name),os.path.join(os.path.join(train_folder, unknown), file_no_extension + \"_\" + folder_name + file_extension))\n",
        "      \n",
        "#Delete empty folders in training folder\n",
        "for folder_name in os.listdir(train_folder):\n",
        "    folder_path = os.path.join(train_folder, folder_name)\n",
        "\n",
        "    if os.path.isdir(folder_path) and not os.listdir(folder_path):\n",
        "       os.rmdir(folder_path)\n",
        "\n",
        "#Append name of folder to every file in test folder, then apply mapping\n",
        "for folder_name in os.listdir(test_folder):\n",
        "\n",
        "    folder_path = os.path.join(test_folder, folder_name)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "      \n",
        "      for file_name in os.listdir(folder_path):\n",
        "          file_extension = pathlib.Path(file_name).suffix\n",
        "          file_no_extension = file_name.replace(file_extension, \"\")\n",
        "          #os.rename (os.path.join(folder_path, file_name),os.path.join(folder_path, file_no_extension + \"_\" + folder_name + file_extension))\n",
        "\n",
        "          if mapping.get(folder_name) is not None:\n",
        "            os.rename (os.path.join(folder_path, file_name),os.path.join(os.path.join(test_folder, mapping[folder_name]), file_no_extension + \"_\" + folder_name + file_extension))\n",
        "          else:\n",
        "            os.rename (os.path.join(folder_path, file_name),os.path.join(os.path.join(test_folder, unknown), file_no_extension + \"_\" + folder_name + file_extension))\n",
        "      \n",
        "#Delete empty folders in test folder\n",
        "for folder_name in os.listdir(test_folder):\n",
        "    folder_path = os.path.join(test_folder, folder_name)\n",
        "    \n",
        "    if os.path.isdir(folder_path) and not os.listdir(folder_path):\n",
        "       os.rmdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "fEbeOkfnAZH2"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((48, 48)), \n",
        "    #transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629)),\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(train_folder,\n",
        "                         transform=data_transforms),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(test_folder,\n",
        "                         transform=data_transforms),\n",
        "    batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVAdz54mBtCs",
        "outputId": "063b3420-d66c-4cfc-f1b0-4ee1f680bd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39209\n"
          ]
        }
      ],
      "source": [
        "print (len(train_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28jd1mqUFfIs",
        "outputId": "8c00e91e-61cb-4644-999b-186db9160edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12569\n"
          ]
        }
      ],
      "source": [
        "print (len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Kv1owgO7R8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "    # Extract the labels from the current batch\n",
        "    batch_labels = labels.numpy()\n",
        "\n",
        "    for i in range (len(labels)):\n",
        "      plt.imshow(images[i].cpu().permute(1,2,0))\n",
        "      plt.show()\n",
        "      print(f\"Label: {labels[i]}\")\n",
        "\n",
        "    #time.sleep (2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "EE8onOZbNsFX"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "numClasses = 13   #12 classes + unknown \n",
        "net = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "net.fc = torch.nn.Linear(net.fc.in_features, numClasses)\n",
        "net = net.to(device)\n",
        "    \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  #lr=0.01, weight_decay=1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yQpCqy6G73i"
      },
      "outputs": [],
      "source": [
        "for epoch in range(2):\n",
        "    running_loss = 0.0\n",
        "    for i, (data, labels) in enumerate(train_loader):\n",
        "        images, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "          print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100)) \n",
        "          running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI6WBqhKuIO5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for i, (data, labels) in enumerate(test_loader):\n",
        "          images, labels = data.to(device), labels.to(device)\n",
        "\n",
        "          outputs = net(images)\n",
        "\n",
        "          pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "          #_, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "          labels = labels.reshape(-1)\n",
        "          pred = pred.reshape(-1)\n",
        "\n",
        "          \n",
        "          #for i in range (len(pred)):\n",
        "            #print (labels[i])\n",
        "            #print (pred[i])\n",
        "            #plt.imshow(images[i].cpu().permute(1,2,0))\n",
        "            #plt.show()\n",
        "          \n",
        "          print (labels)\n",
        "          print (pred)\n",
        "\n",
        "          correct += (pred == labels).float().sum()\n",
        "          total += len(pred)\n",
        "\n",
        "          print (\"Correct: \" + str((pred == labels).float().sum()))\n",
        "          #time.sleep (10)\n",
        "\n",
        "  '''\n",
        "          print (\"Correct : \" + str(correct))\n",
        "          print (\"Label:\")\n",
        "          print (labels)\n",
        "          print (\"Pred:\")\n",
        "          print (pred)\n",
        "          time.sleep(10)\n",
        "  '''\n",
        "\n",
        "  print (\"Total: \" + str(total))\n",
        "  print (\"Test_loader.dataset: \" + str(len(test_loader.dataset)))\n",
        "  print (\"Test Accuracy: \" + str (100*(correct/len(test_loader.dataset))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "yOp9l1BnXeuR"
      },
      "outputs": [],
      "source": [
        "!rm -rf \"/content/drive/My Drive/train_images/\"\n",
        "!rm -rf \"/content/drive/My Drive/test_images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlBOswti78qZ"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "GBoWqZ45M00w"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/prova'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWPckoO5M9lY"
      },
      "outputs": [],
      "source": [
        "!pip3 install deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eef9aXinMsKS"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "\n",
        "train = deeplake.load(\"hub://activeloop/gtsrb-train\")\n",
        "test = deeplake.load(\"hub://activeloop/gtsrb-test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xSJd-XSIsQT"
      },
      "outputs": [],
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((48, 48)), \n",
        "    #transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629)),\n",
        "])\n",
        "\n",
        "print (train.tensors.keys())\n",
        "\n",
        "'''\n",
        "\n",
        "for i in range (0, len(train)):\n",
        "        train.labels[i] = 100 \n",
        "\n",
        "for i, data in enumerate(train):\n",
        "        image, label = data['images'], data['labels']\n",
        "        print (label)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_loader = train.pytorch(num_workers=0, batch_size=64, transform={\n",
        "                        'images': tform, 'labels': None}, shuffle=False)\n",
        "\n",
        "test_loader = test.pytorch(num_workers=0, batch_size=64, transform={\n",
        "                        'images': tform, 'labels': None}, shuffle=False)\n",
        "'''\n",
        "for i in range(0, len(train)):\n",
        "        train.labels[1] = 100\n",
        "        #images, labels = train.images[i], train.labels[i]\n",
        "\n",
        "for i, data in enumerate(train):\n",
        "        image, label = data['images'], data['labels']\n",
        "        print (label)\n",
        "        '''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}